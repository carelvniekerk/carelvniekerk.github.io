---
permalink: /
title: "Carel van Niekerk"
author_profile: true
redirect_from:
    - /about/
    - /about.html
---

# About Me

I am a Postdoctoral Researcher in the [Dialogue Systems and Machine Learning](https://www.cs.hhu.de/en/research-groups/dialog-systems-and-machine-learning.html) group at Heinrich Heine University, Düsseldorf, under Prof. Milica Gašić. My current research, part of the ‘Lamarr Fellow Network Ramp Up’ project, bridges rigorous mathematical theory with production-level engineering to advance **Reinforcement Learning (RL) Post-training** and Large Language Model (LLM) Alignment. I recently received my PhD with magna cum laude, focusing on **uncertainty estimation** in human-computer dialogue.

I possess over 10 years of experience across academia and industry. My core expertise lies in designing scalable, reliable, and modular training frameworks. I am a **proficient Python developer** with experience in C++ and Rust, utilising the PyTorch and Hugging Face ecosystems (Transformers, TRL, Accelerate) to build robust deep learning infrastructure. I have architected tools such as [HydraXcel](https://github.com/carelvniekerk/HydraXcel), an open-source experiment launcher integrating Facebook Hydra, Hugging Face Accelerate, and the UV workflow to support scalable multi-GPU and **distributed training**. Additionally, I was a core developer of ConvLab-3, a unified dialogue system toolkit adopted in over 30 research papers.

My work in agentic systems includes developing **Multi-Agent Reinforcement Learning (MARL)** frameworks that have improved routing accuracy by over 15% in production-level dialogue products. I have also designed active learning strategies that achieved full-dataset performance using only 16% of expert annotations. Previously, I co-authored the 2020 SIGDial best paper “TripPy” and worked as an AI consultant at NGA Risksecure, where I delivered quantitative metrics for banking clients.

I regularly publish at leading venues including **NeurIPS**, **ACL**, and **EMNLP**. My recent contributions include identifying Local Intrinsic Dimensions as predictive metrics for model training dynamics (NeurIPS 2025) and developing Reinforcement Learning from Self-Feedback (RLSF). Beyond research, I am committed to **mentorship**, having designed and taught the "Implementing Transformers" course—guiding students to build architectures from first principles—and successfully supervising numerous Master’s theses.

## Highlights

-   **1 October 2025** - Our paper, [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://carelvniekerk.github.io/publication/2025-less-is-more), has been accepted for presentation at _NeurIPS 2025_. In this work, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning in large language models (LLMs). We show that local dimensions provide insights into a model's training dynamics and generalization ability, predicting phenomena such as training exhaustion, overfitting, and grokking across various tasks. Our findings offer practical heuristics for model configuration and contribute to the discourse on LLM interpretability, adaptability, and generalizability by bridging intrinsic model mechanisms with geometric properties in embeddings.
-   **29 July 2025** - We have released a preprint of our paper, [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://carelvniekerk.github.io/publication/2025-rlsf). In this work, we introduce Reinforcement Learning from Self-Feedback (RLSF), a novel post-training approach that leverages a model's own confidence as an intrinsic reward. This method enhances both the calibration and reasoning capabilities of Large Language Models (LLMs) without the need for human labels or external rewards, marking a significant advancement in LLM post-training techniques.
-   **1 July 2025** - Our paper, [A Confidence-based Acquisition Model for Self-supervised Active Learning and Label Correction](https://carelvniekerk.github.io/publication/2025-camell), has been published in _Transactions of the Association for Computational Linguistics (TACL)_ and will be presented at _ACL 2025_ in Vienna. In this work, we introduce CAMEL, a novel active learning framework designed for sequential multi-output tasks, which significantly enhances efficiency and data quality through partial expert labeling and self-supervision.
-   **9 April 2024** - Today I recieved my Ph.D. with magna cum laude. My thesis, titled "Uncertainty Estimation, Management, and Utilisation in Human-Computer Dialogue", explores the role of uncertainty in Language Understanding and its implications for dialogue systems. I am grateful for the support of my advisor, Prof. Milica Gašić, and the members of the Dialogue Systems and Machine Learning group at Heinrich Heine University.
